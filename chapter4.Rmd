# Week 4 exercises

Didn't have time to finish these this week! Less work vfor the reviewer I guerss. I did a few of the starting points, and will finalize these later.

## The Boston dataset

```{r}
library(MASS)
data("Boston")
str(Boston)
dim(Boston)
summary(Boston)
```

The dataset has various socio-political data of Boston suburbs. The data varies from crime rates to property values and student/teacher ratios in schools.

## Dataset summary

```{r}
library(tidyr)
library(corrplot)
library(ggplot2)
library(GGally)

cor_matrix <- cor(Boston) %>% round(2) 
cor_matrix 

ggpairs(Boston)

corrplot(cor_matrix, method="circle", type="upper")


Boston %>% ggplot(aes(indus)) +
  geom_histogram(binwidth = 1) + 
  ylab("Industry ratio")

Boston %>% ggplot(aes(rad)) +
  geom_histogram(binwidth = 1) + 
  ylab("Radial highway access")

Boston %>% ggplot(aes(tax)) +
  geom_histogram(binwidth = 20) +
  ylab("Property tax rate")

Boston %>% ggplot(aes(age)) +
  geom_histogram(binwidth = 10) +
  ylab("Building age")



```
Few variables have a very strongly stratified scope. Some suburbs have very high values, most others very low. See for example crime rate (crim), river proximity (chas) and proportion of people of colour of the residents (black).

Industry rate seems to correlate (nagatively or positively) strongly with many of the other variables in the dataset. The histograms of the correlating variables vary in shape quite widely, so probably there's quite a bit of variance in suburbs on these.

## Standardized data, training and testing sets

```{r}
library(dplyr)

boston_scaled <- scale(Boston) %>% as.data.frame()
boston_scaled$crim <- as.numeric(boston_scaled$crim)
summary(boston_scaled)

crime <- cut(boston_scaled$crim, breaks = quantile(boston_scaled$crim), include.lowest = TRUE,
             labels = c("low", "med_low", "med_high", "high"))

boston_scaled$crim <- crime

# Create test and train datasets
ind <- sample(nrow(boston_scaled),  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```

All the variables were standardized with the scale() -function. The "crim" (crime rate) variable in the Boston dataset was overwritten with the standardized categorized version.

## LDA

```{r}
lda.fit <- lda(crim ~ ., data = train)
lda.fit


lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}


classes <- as.numeric(train$crim)

plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```

## LDA predictions

```{r}
test_crime <- test$crim

test_lda <- dplyr::select(test, -crim)

lda.pred <- predict(lda.fit, newdata = test_lda)
table(correct = test$crim, predicted = lda.pred$class)
```

Looks like the 'low' -category got mixed predictions of low and med_low, while in the other categories the predictions fit the test data correctly. The sample size is quite small, and the actual values of the categorized variable might be close to the dividing line in the low/med_low area. Or the explaining variables might not cover all the actual reasons for high crime rates. It's very unlikely that they would.

## K-means (unfinished)

```{r}
boston_scaled <- scale(Boston) %>% as.data.frame()
```



Reload the Boston dataset and standardize the dataset (we did not do this in the Exercise Set, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results. (0-4 points)
